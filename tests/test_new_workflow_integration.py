"""
Integration test for the new bank CSV workflow:
process-jcb/process-ufj -> process -> confirm
"""

import sys
import os
sys.path.append('/workspace')

from ledger_ingest.processor import CSVProcessor
from ledger_ingest.config import PROCESS_DIR, CONFIRMED_DIR
import pandas as pd
from pathlib import Path
import tempfile
import pytest


def test_new_bank_workflow():
    """Test new bank CSV workflow: process_bank_csv -> process -> confirm"""
    
    print("=== æ–°éŠ€è¡ŒCSVãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    # 1. CSVProcessoråˆæœŸåŒ–
    processor = CSVProcessor()
    print("âœ“ CSVProcessoråˆæœŸåŒ–å®Œäº†")
    
    # 2. ãƒ†ã‚¹ãƒˆç”¨UFJãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    ufj_test_data = """å–å¼•æ—¥,æ‘˜è¦,æ‘˜è¦å†…å®¹,æ”¯æ‰•é‡‘é¡,å—å–é‡‘é¡,æ®‹é«˜,memo,å–å¼•åº—å,å–å¼•åº—ç•ªå·
2024-03-01,æŒ¯è¾¼å…¥é‡‘,ãƒ†ã‚¹ãƒˆå…¥é‡‘,,5000,15000,,,
2024-03-02,ãƒ‡ãƒ“ãƒƒãƒˆ1,ãƒ†ã‚¹ãƒˆã‚·ãƒ§ãƒƒãƒ—,1500,,13500,,,
2024-03-03,æŒ¯è¾¼å‡ºé‡‘,å®¶è³ƒæ”¯æ‰•ã„,60000,,53500,,,"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='shift_jis') as f:
        f.write(ufj_test_data)
        f.flush()
        ufj_test_file = f.name
    
    try:
        # 3. process_bank_csv ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆä¸­é–“CSVç”Ÿæˆï¼‰
        print("\n--- Step 1: Bank CSVå‡¦ç† (ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ) ---")
        
        # PROCESS_DIRã‚’ã‚¯ãƒªã‚¢
        for file in PROCESS_DIR.glob('*.csv'):
            file.unlink()
        
        # UFJ CSVå‡¦ç†
        processed_count = processor.process_bank_csv(ufj_test_file, 'ufj', clear_temp=True, check_duplicates=False)
        print(f"âœ“ UFJ CSVå‡¦ç†å®Œäº†: {processed_count}ä»¶ã®ä»•è¨³")
        
        # ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        process_files = list(PROCESS_DIR.glob('*.csv'))
        assert len(process_files) > 0, "ä¸­é–“CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“"
        print(f"âœ“ ä¸­é–“CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆç¢ºèª: {len(process_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ç¢ºèª
        for file in process_files:
            df = pd.read_csv(file)
            print(f"  - {file.name}: {len(df)}è¡Œ")
            assert len(df) > 0, f"ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ« {file.name} ãŒç©ºã§ã™"
            required_columns = ['Date', 'SetID', 'EntryID', 'SubjectCode', 'Amount', 'Remarks', 'Subject']
            missing_columns = set(required_columns) - set(df.columns)
            assert len(missing_columns) == 0, f"å¿…è¦ãªåˆ—ãŒä¸è¶³: {missing_columns}"
        
        # 4. process ã‚³ãƒãƒ³ãƒ‰ã§ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        print("\n--- Step 2: Process ã‚³ãƒãƒ³ãƒ‰ (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²) ---")
        
        # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        first_process_file = process_files[0]
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‡¦ç†
        db_processed_count = processor.process_csv_for_database(str(first_process_file))
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‡¦ç†å®Œäº†: {db_processed_count}ä»¶ã®ä»•è¨³ã‚’temp_journalã«ç™»éŒ²")
        
        # ã‚»ãƒƒãƒˆæ¤œè¨¼
        is_valid, message, errors = processor.validate_sets()
        print(f"âœ“ ã‚»ãƒƒãƒˆæ¤œè¨¼: {message}")
        assert is_valid, f"ã‚»ãƒƒãƒˆæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {message}"
        
        # 5. confirm ã‚³ãƒãƒ³ãƒ‰ã§ä»•è¨³ç¢ºå®š
        print("\n--- Step 3: Confirm ã‚³ãƒãƒ³ãƒ‰ (ä»•è¨³ç¢ºå®š) ---")
        
        # ç¢ºå®šå‰ã®process/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ç¢ºèª
        process_files_before = list(PROCESS_DIR.glob('*.csv'))
        process_count_before = len(process_files_before)
        
        # ç¢ºå®šå‡¦ç†
        confirm_success = processor.confirm_entries()
        print(f"âœ“ ä»•è¨³ç¢ºå®š: {'æˆåŠŸ' if confirm_success else 'å¤±æ•—'}")
        assert confirm_success, "ä»•è¨³ç¢ºå®šã«å¤±æ•—ã—ã¾ã—ãŸ"
        
        # ç¢ºå®šå¾Œã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
        process_files_after = list(PROCESS_DIR.glob('*.csv'))
        confirmed_files = list(CONFIRMED_DIR.glob('*.csv'))
        
        print(f"âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ç¢ºèª:")
        print(f"  - process/: {process_count_before} â†’ {len(process_files_after)}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"  - confirmed/: {len(confirmed_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãç§»å‹•ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        assert len(process_files_after) == 0, "process/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ®‹ã£ã¦ã„ã¾ã™"
        assert len(confirmed_files) >= process_count_before, "confirmed/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒç§»å‹•ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # 6. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹ã®ç¢ºèª
        print("\n--- Step 4: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹ç¢ºèª ---")
        
        # temp_journalãŒã‚¯ãƒªã‚¢ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        temp_summary = processor.get_transaction_summary()
        print(f"âœ“ temp_journalçŠ¶æ…‹: {len(temp_summary)}ä»¶")
        assert len(temp_summary) == 0, "temp_journalãŒã‚¯ãƒªã‚¢ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        # è©¦ç®—è¡¨ã®ç¢ºèª
        try:
            trial_balance = processor.get_trial_balance()
            print(f"âœ“ è©¦ç®—è¡¨å–å¾—: {len(trial_balance)}è¡Œ")
        except Exception as e:
            print(f"â„¹ è©¦ç®—è¡¨å–å¾—ã‚¨ãƒ©ãƒ¼ï¼ˆæƒ³å®šå†…ï¼‰: {e}")
        
        print("\n=== æ–°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº† ===")
        print("âœ“ process_bank_csv â†’ process â†’ confirm ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒæ­£å¸¸ã«å‹•ä½œ")
        print("âœ“ ä¸­é–“CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆãƒ»å‡¦ç†ãƒ»ç§»å‹•ãŒæ­£å¸¸ã«å®Ÿè¡Œ")
        print("âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œãŒæ­£å¸¸ã«å®Ÿè¡Œ")
        print("âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ãŒæ­£å¸¸ã«å®Ÿè¡Œ")
        
        assert True
        
    finally:
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            os.unlink(ufj_test_file)
        except:
            pass
        
        # process/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for file in PROCESS_DIR.glob('*.csv'):
            try:
                file.unlink()
            except:
                pass


def test_bank_csv_no_database_operations():
    """Test that process_bank_csv does not perform database operations"""
    
    print("=== process_bank_csvã®DBéæ“ä½œãƒ†ã‚¹ãƒˆ ===")
    
    processor = CSVProcessor()
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿
    test_data = """å–å¼•æ—¥,æ‘˜è¦,æ‘˜è¦å†…å®¹,æ”¯æ‰•é‡‘é¡,å—å–é‡‘é¡,æ®‹é«˜
2024-03-01,ãƒ†ã‚¹ãƒˆ,ãƒ†ã‚¹ãƒˆå–å¼•,1000,,9000"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='shift_jis') as f:
        f.write(test_data)
        f.flush()
        test_file = f.name
    
    try:
        # PROCESS_DIRã‚’ã‚¯ãƒªã‚¢
        for file in PROCESS_DIR.glob('*.csv'):
            file.unlink()
        
        # æœ€åˆã®temp_journalçŠ¶æ…‹ã‚’ç¢ºèª
        initial_summary = processor.get_transaction_summary()
        initial_count = len(initial_summary)
        
        # process_bank_csvã‚’å®Ÿè¡Œ
        processed_count = processor.process_bank_csv(test_file, 'ufj', clear_temp=False, check_duplicates=False)
        
        # temp_journalã®çŠ¶æ…‹ã‚’å†ç¢ºèª
        final_summary = processor.get_transaction_summary()
        final_count = len(final_summary)
        
        print(f"âœ“ temp_journalçŠ¶æ…‹: {initial_count} â†’ {final_count}ä»¶")
        print(f"âœ“ å‡¦ç†ã•ã‚ŒãŸä»•è¨³: {processed_count}ä»¶")
        
        # process_bank_csvãŒtemp_journalã«æ›¸ãè¾¼ã‚“ã§ã„ãªã„ã“ã¨ã‚’ç¢ºèª
        assert initial_count == final_count, "process_bank_csvãŒtemp_journalã«æ›¸ãè¾¼ã‚“ã§ã„ã¾ã™"
        
        # ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        process_files = list(PROCESS_DIR.glob('*.csv'))
        assert len(process_files) > 0, "ä¸­é–“CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        print("âœ“ process_bank_csvã¯DBæ“ä½œã‚’è¡Œã‚ãšã€ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã®ã¿å®Ÿè¡Œ")
        
    finally:
        try:
            os.unlink(test_file)
        except:
            pass
        
        # process/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for file in PROCESS_DIR.glob('*.csv'):
            try:
                file.unlink()
            except:
                pass


if __name__ == "__main__":
    test_new_bank_workflow()
    test_bank_csv_no_database_operations()
    print("\nğŸ‰ æ–°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å…¨ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")